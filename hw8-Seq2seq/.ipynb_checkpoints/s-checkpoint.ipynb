{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html?fbclid=IwAR3uZ9em4uzZ5hH8DB7VQrJSyy0aFnoKvUdu4zW_K_W2ZIjBEAmPqljqXw4\n",
    "\n",
    "看不懂的話，先找比較簡單的範例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-19T17:21:03.001653Z",
     "start_time": "2020-12-19T17:20:58.762720Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import torch.utils.data.sampler as sampler\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import random\n",
    "import json\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # 判斷是用 CPU 還是 GPU 執行運算\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-19T16:00:34.244313Z",
     "start_time": "2020-12-19T16:00:34.237059Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data_dirname = '/workdir/home/feynman52/NTU-ML2020/hw8-Seq2seq/datasets/cmn-eng'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-19T16:00:36.228060Z",
     "start_time": "2020-12-19T16:00:36.153585Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "filename = 'training.txt'\n",
    "filepath = os.path.join(data_dirname, filename)\n",
    "with open(filepath) as f:\n",
    "    content = f.readlines()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-19T16:00:38.817891Z",
     "start_time": "2020-12-19T16:00:38.792723Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "content[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-13T12:51:35.394151Z",
     "start_time": "2020-12-13T12:51:35.216Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "'0': '<PAD>',\n",
    "'1': '<BOS>',\n",
    "'2': '<EOS>',\n",
    "'3': '<UNK>',\n",
    "\n",
    "'<PAD>': 0,\n",
    "'<BOS>': 1,\n",
    "'<EOS>': 2,\n",
    "'<UNK>': 3,\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-19T16:00:43.045330Z",
     "start_time": "2020-12-19T16:00:42.986157Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# filename = 'int2word_cn.json'\n",
    "filename = 'int2word_en.json'\n",
    "# filename = 'word2int_cn.json'\n",
    "# filename = 'word2int_en.json'\n",
    "\n",
    "filepath = os.path.join(data_dirname, filename)\n",
    "filepath\n",
    "with open(filepath, \"r\") as f:\n",
    "    int2word_en = json.load(f)\n",
    "\n",
    "# int2word_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-19T16:00:45.457235Z",
     "start_time": "2020-12-19T16:00:45.444090Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# filename = 'int2word_cn.json'\n",
    "# filename = 'int2word_en.json'\n",
    "# filename = 'word2int_cn.json'\n",
    "filename = 'word2int_en.json'\n",
    "\n",
    "filepath = os.path.join(data_dirname, filename)\n",
    "filepath\n",
    "with open(filepath, \"r\") as f:\n",
    "    word2int_en = json.load(f)\n",
    "\n",
    "# word2int_en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-19T16:00:47.283422Z",
     "start_time": "2020-12-19T16:00:47.274663Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sentence = content[1].replace(' \\n', '')\n",
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-19T16:00:48.972155Z",
     "start_time": "2020-12-19T16:00:48.966519Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "en, cn = sentence.split(' \\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-19T16:00:51.334196Z",
     "start_time": "2020-12-19T16:00:51.324973Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "en_word = en.split(' ')\n",
    "en_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-19T16:00:53.494624Z",
     "start_time": "2020-12-19T16:00:53.485571Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cn_word = cn.split(' ')\n",
    "cn_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-19T16:00:58.243956Z",
     "start_time": "2020-12-19T16:00:58.233141Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "en_word = ['how', 'are', 'you', 'fuck']\n",
    "en_int = [word2int_en[word] if word in word2int_en else word2int_en['<UNK>'] for word in en_word]\n",
    "en_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-19T16:01:05.448400Z",
     "start_time": "2020-12-19T16:01:05.442782Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "seq_len = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-19T16:01:06.927317Z",
     "start_time": "2020-12-19T16:01:06.916857Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pad_en_int = en_int + [word2int_en['<PAD>'] for i in range(seq_len-len(en_int))]\n",
    "pad_en_int"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-19T17:37:35.514865Z",
     "start_time": "2020-12-19T17:37:35.486577Z"
    }
   },
   "outputs": [],
   "source": [
    "class MyDataset(data.Dataset):\n",
    "    def __init__(self, \n",
    "                 data_dirname='./datasets/cmn-eng',\n",
    "                 seq_len=20, \n",
    "                 txt_filename='training.txt'):\n",
    "        \n",
    "        self.data_dirname = data_dirname\n",
    "        self.seq_len = seq_len\n",
    "        self.txt_filename = txt_filename\n",
    "        self.setup_json()\n",
    "        self.setup_data()\n",
    "        \n",
    "    def setup_data(self):\n",
    "        self.en_sentence_list = []\n",
    "        self.cn_sentence_list = []\n",
    "        \n",
    "        self.en_cn_list = self.load_txt()\n",
    "        for en_cn in self.en_cn_list:\n",
    "            en_cn = en_cn.replace(' \\n', '')\n",
    "            en_text, cn_text = en_cn.split(' \\t')\n",
    "            en_word_list = en_text.split(' ')\n",
    "            cn_word_list = cn_text.split(' ')\n",
    "            en_int_list = self.word_list_to_int_list(en_word_list, self.word2int_en)\n",
    "            cn_int_list = self.word_list_to_int_list(cn_word_list, self.word2int_cn)\n",
    "            \n",
    "            self.en_sentence_list.append(en_int_list)\n",
    "            self.cn_sentence_list.append(cn_int_list)\n",
    "    \n",
    "    def word_list_to_int_list(self, word_list=['a', 'b'], word2inc_dict={}):\n",
    "        UNK = word2inc_dict['<UNK>']\n",
    "        PAD = word2inc_dict['<PAD>']\n",
    "        word_list = ['<BOS>'] + word_list + ['<EOS>']\n",
    "        int_list = [word2inc_dict[word] if word in word2inc_dict else UNK for word in word_list]\n",
    "        if self.seq_len >= len(int_list):\n",
    "            pad_int_list = int_list + [PAD for i in range(self.seq_len-len(int_list))]\n",
    "        else:\n",
    "            pad_int_list = int_list[:self.seq_len]\n",
    "            \n",
    "        pad_int_list = torch.LongTensor(pad_int_list)\n",
    "        return pad_int_list\n",
    "    \n",
    "    def int_list_to_word_list(self, int_list=[1, 2], int2word_dict={}):\n",
    "        word_list = [int2word_dict[str(i)] for i in int_list]\n",
    "        return word_list\n",
    "    \n",
    "    def load_txt(self):\n",
    "        filepath = os.path.join(self.data_dirname, self.txt_filename)\n",
    "        with open(filepath) as f:\n",
    "            content = f.readlines()\n",
    "        return content\n",
    "    \n",
    "    def load_json(self, filename='int2word_cn.json'):\n",
    "        filepath = os.path.join(self.data_dirname, filename)\n",
    "        with open(filepath, \"r\") as f:\n",
    "            word2int_en = json.load(f)\n",
    "        return word2int_en\n",
    "    \n",
    "    def setup_json(self):\n",
    "        self.int2word_en = self.load_json(filename='int2word_en.json')\n",
    "        self.int2word_cn = self.load_json(filename='int2word_cn.json')\n",
    "        \n",
    "        self.word2int_en = self.load_json(filename='word2int_en.json')\n",
    "        self.word2int_cn = self.load_json(filename='word2int_cn.json')\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.en_sentence_list)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        en_sentence = self.en_sentence_list[index]\n",
    "        cn_sentence = self.cn_sentence_list[index]\n",
    "        return en_sentence, cn_sentence\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-19T17:37:38.492203Z",
     "start_time": "2020-12-19T17:37:38.159975Z"
    }
   },
   "outputs": [],
   "source": [
    "myDataset = MyDataset(seq_len=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-19T17:40:03.180725Z",
     "start_time": "2020-12-19T17:40:03.169943Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1,  8, 33, 15,  3,  2,  0,  0,  0,  0])\n",
      "['<BOS>', '你', '好', '嗎', '<UNK>', '<EOS>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n"
     ]
    }
   ],
   "source": [
    "word_list = ['你', '好', '嗎', '鑫']\n",
    "\n",
    "int_list = myDataset.word_list_to_int_list(word_list, myDataset.word2int_cn)\n",
    "print(int_list)\n",
    "\n",
    "word_list_ = myDataset.int_list_to_word_list(int_list.tolist(), myDataset.int2word_cn)\n",
    "print(word_list_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-19T17:40:11.090564Z",
     "start_time": "2020-12-19T17:40:11.079709Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1, 52, 30,  8,  3,  2,  0,  0,  0,  0])\n",
      "['<BOS>', 'how', 'are', 'you', '<UNK>', '<EOS>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n"
     ]
    }
   ],
   "source": [
    "word_list = ['how', 'are', 'you', 'fuck']\n",
    "\n",
    "int_list = myDataset.word_list_to_int_list(word_list, myDataset.word2int_en)\n",
    "print(int_list)\n",
    "\n",
    "word_list_ = myDataset.int_list_to_word_list(int_list.tolist(), myDataset.int2word_en)\n",
    "print(word_list_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-19T17:40:13.558436Z",
     "start_time": "2020-12-19T17:40:13.549481Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18000, 18000)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(myDataset.en_sentence_list), len(myDataset.cn_sentence_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-19T17:40:25.220569Z",
     "start_time": "2020-12-19T17:40:25.209759Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "he is a teacher . \t他 是 老師 。 \n",
      "\n",
      "['<BOS>', 'he', 'is', 'a', 'teacher', '.', '<EOS>', '<PAD>', '<PAD>', '<PAD>']\n",
      "['<BOS>', '他', '是', '老師', '。', '<EOS>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n"
     ]
    }
   ],
   "source": [
    "i = 2\n",
    "print(myDataset.en_cn_list[i])\n",
    "\n",
    "en_int_list, cn_int_list = myDataset[i]\n",
    "\n",
    "en_word_list = myDataset.int_list_to_word_list(en_int_list.tolist(), myDataset.int2word_en)\n",
    "print(en_word_list)\n",
    "\n",
    "cn_word_list = myDataset.int_list_to_word_list(cn_int_list.tolist(), myDataset.int2word_cn)\n",
    "print(cn_word_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-19T17:40:41.849254Z",
     "start_time": "2020-12-19T17:40:41.310300Z"
    }
   },
   "outputs": [],
   "source": [
    "train_set = MyDataset(txt_filename='training.txt')\n",
    "valid_set = MyDataset(txt_filename='validation.txt')\n",
    "test_set = MyDataset(txt_filename='testing.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-19T17:40:43.334011Z",
     "start_time": "2020-12-19T17:40:43.310754Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "360"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 50\n",
    "train_loader = data.DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-19T17:41:11.891489Z",
     "start_time": "2020-12-19T17:41:11.879922Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 20]) torch.Size([50, 20])\n"
     ]
    }
   ],
   "source": [
    "x, y = next(iter(train_loader))\n",
    "print(x.shape, y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-19T19:13:43.919696Z",
     "start_time": "2020-12-19T19:13:43.906156Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.2929,  0.4070,  0.0852],\n",
       "         [ 0.8098, -0.3312,  0.2276]]),\n",
       " tensor([[0.3409, 0.3821, 0.2770],\n",
       "         [0.5324, 0.1701, 0.2974]]))"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# m = nn.LogSoftmax(dim=1)\n",
    "m = nn.Softmax(dim=1)\n",
    "\n",
    "input = torch.randn(2, 3)\n",
    "output = m(input)\n",
    "\n",
    "input, output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AttDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
