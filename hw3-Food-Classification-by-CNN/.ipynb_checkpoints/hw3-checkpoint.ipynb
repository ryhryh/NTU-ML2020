{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-07T14:28:21.828389Z",
     "start_time": "2020-06-07T14:28:20.662231Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import time\n",
    "from glob import glob\n",
    "import re\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]='2' \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read data\n",
    "- x_train_paths, y_train\n",
    "- x_valid_paths, y_valid\n",
    "- x_test_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-07T14:28:21.834857Z",
     "start_time": "2020-06-07T14:28:21.830400Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_paths_and_labels(img_type='training', isLbael=True):\n",
    "    base_dir = '/workdir/home/feynman52/NTU-ML2020/hw3-Food-Classification-by-CNN/datasets'\n",
    "    paths = sorted(glob(os.path.join(base_dir, img_type, '*')))[:]\n",
    "    \n",
    "    if isLbael==True: \n",
    "        Y = [int(re.search('/(.{1,2})_', path).group(1)) for path in paths]\n",
    "        return paths, Y\n",
    "    else:\n",
    "        return paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-07T14:28:22.243227Z",
     "start_time": "2020-06-07T14:28:21.837174Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9866, 9866, 3430, 3430, 3347)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_paths, y_train = get_paths_and_labels(img_type='training', isLbael=True)\n",
    "x_valid_paths, y_valid = get_paths_and_labels(img_type='validation', isLbael=True)\n",
    "x_test_paths = get_paths_and_labels(img_type='testing', isLbael=False)\n",
    "\n",
    "len(x_train_paths), len(y_train), len(x_valid_paths), len(y_valid), len(x_test_paths)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-07T14:28:22.250914Z",
     "start_time": "2020-06-07T14:28:22.245971Z"
    }
   },
   "outputs": [],
   "source": [
    "# training 時做 data augmentation\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.RandomHorizontalFlip(), # 隨機將圖片水平翻轉\n",
    "    transforms.RandomRotation(15), # 隨機旋轉圖片\n",
    "    transforms.ToTensor(), # 將圖片轉成 Tensor，並把數值 normalize 到 [0,1] (data normalization)\n",
    "])\n",
    "# testing 時不需做 data augmentation\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),                                    \n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### test\n",
    "https://pytorch.org/docs/stable/torchvision/transforms.html"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T05:38:50.192318Z",
     "start_time": "2020-05-26T05:38:50.170883Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "source": [
    "# https://discuss.pytorch.org/t/understanding-transform-normalize/21730\n",
    "\n",
    "transform1 = transforms.Compose(\n",
    "    [\n",
    "        # [0,1]\n",
    "        transforms.ToTensor(), \n",
    "    ]\n",
    ")\n",
    "\n",
    "transform2 = transforms.Compose(\n",
    "    [\n",
    "        # 3 channels\n",
    "        # [(0-0.5)/0.5, (1-0.5)/0.5] = [-1,-1]\n",
    "        transforms.Normalize(mean = (0.5, 0.5, 0.5), std = (0.5, 0.5, 0.5)) \n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "img = np.arange(3).reshape(1,1,3).astype('uint8') ### astype('uint8')\n",
    "img1 = transform1(img)  \n",
    "img2 = transform2(img1)\n",
    "\n",
    "\n",
    "print(img.shape, img1.shape, img2.shape)\n",
    "\n",
    "print('#'*50)\n",
    "print(img)\n",
    "\n",
    "print('#'*50)\n",
    "print(img1) # 0/255=0, 1/255=0.0039, 2/255=0.0078\n",
    "\n",
    "print('#'*50)\n",
    "print(img2) # (0-0.5)/0.5=0, (0.0039-0.5)/0.5=-0.9922, (0.0078-0.5)/0.5=-0.9844, \n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T05:46:08.350845Z",
     "start_time": "2020-05-26T05:46:08.330899Z"
    },
    "hidden": true
   },
   "source": [
    "transform1 = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor()\n",
    "    ]\n",
    ")\n",
    "\n",
    "transform2 = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToPILImage(),\n",
    "        \n",
    "        # input image, so need to transfrom to transfrom to image previously\n",
    "        transforms.RandomHorizontalFlip(), \n",
    "        \n",
    "        transforms.ToTensor()\n",
    "    ]\n",
    ")\n",
    "\n",
    "img = np.arange(4).reshape(2,2,1).astype('uint8') ### astype('uint8')\n",
    "img1 = transform1(img)\n",
    "img2 = transform2(img)\n",
    "\n",
    "\n",
    "print(img.shape, img1.shape, img2.shape)\n",
    "\n",
    "print('#'*50)\n",
    "print(img)\n",
    "\n",
    "print('#'*50)\n",
    "print(img1)\n",
    "\n",
    "print('#'*50)\n",
    "print(img2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T05:47:16.081416Z",
     "start_time": "2020-05-26T05:47:16.060053Z"
    },
    "hidden": true
   },
   "source": [
    "transform1 = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor()\n",
    "    ]\n",
    ")\n",
    "\n",
    "transform2 = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToPILImage(),\n",
    "        \n",
    "        # input image, so need to transfrom to transfrom to image previously\n",
    "        transforms.RandomRotation(90),\n",
    "        \n",
    "        transforms.ToTensor()\n",
    "    ]\n",
    ")\n",
    "\n",
    "img = np.arange(4).reshape(2,2,1).astype('uint8') ### astype('uint8')\n",
    "img1 = transform1(img)\n",
    "img2 = transform2(img)\n",
    "\n",
    "\n",
    "print(img.shape, img1.shape, img2.shape)\n",
    "\n",
    "print('#'*50)\n",
    "print(img)\n",
    "\n",
    "print('#'*50)\n",
    "print(img1)\n",
    "\n",
    "print('#'*50)\n",
    "print(img2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "hidden": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-07T14:28:22.294714Z",
     "start_time": "2020-06-07T14:28:22.252806Z"
    }
   },
   "outputs": [],
   "source": [
    "class ImgDataset(Dataset):\n",
    "    def __init__(self, img_paths, labels=None, transform=None):\n",
    "        self.img_paths = img_paths\n",
    "        \n",
    "        self.labels = labels\n",
    "        if self.labels != None:\n",
    "            self.labels = torch.LongTensor(labels) ###\n",
    "            \n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        img_path = self.img_paths[index]\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.resize(img, (128, 128))\n",
    "        \n",
    "        if self.transform!=None: img = self.transform(img)\n",
    "            \n",
    "        if self.labels==None: \n",
    "            return img\n",
    "        else:\n",
    "            label = self.labels[index]\n",
    "            return img, label\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-07T14:28:22.344536Z",
     "start_time": "2020-06-07T14:28:22.298860Z"
    }
   },
   "outputs": [],
   "source": [
    "train_set = ImgDataset(x_train_paths, y_train, train_transform)\n",
    "valid_set = ImgDataset(x_valid_paths, y_valid, test_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-07T14:28:22.450932Z",
     "start_time": "2020-06-07T14:28:22.353837Z"
    }
   },
   "outputs": [],
   "source": [
    "x, y = train_set[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-07T14:28:27.012350Z",
     "start_time": "2020-06-07T14:28:27.003108Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-28T11:46:04.408618Z",
     "start_time": "2020-05-28T11:46:04.370018Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True) # shuffle select index\n",
    "valid_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-28T11:46:04.906500Z",
     "start_time": "2020-05-28T11:46:04.413047Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([50, 3, 128, 128]), torch.Size([50]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = iter(train_loader)\n",
    "x_batch, y_batch = next(g)\n",
    "x_batch.shape, y_batch.shape\n",
    "x_batch.size(), y_batch.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-28T11:46:04.912387Z",
     "start_time": "2020-05-28T11:46:04.908351Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "198"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader) # batch_num = round(9866//50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-28T11:46:04.961045Z",
     "start_time": "2020-05-28T11:46:04.914788Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-27T17:49:25.783195Z",
     "start_time": "2020-05-27T17:49:25.762407Z"
    },
    "hidden": true
   },
   "source": [
    "class ToyDataset(Dataset):\n",
    "    def __init__(self, X=[1,2,3,4,5], Y=[10,20,30,40,50]):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        x = self.X[idx]\n",
    "        y = self.Y[idx]\n",
    "        return x, y \n",
    "        \n",
    "toy_set = ToyDataset()    \n",
    "toy_loader = DataLoader(toy_set, batch_size=2, shuffle=True)   \n",
    "\n",
    "batch_num = len(toy_loader) # round(len(X)//batch_size) \n",
    "print(batch_num)\n",
    "\n",
    "for (i, data) in enumerate(toy_loader):\n",
    "    print(i, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-28T11:46:05.007603Z",
     "start_time": "2020-05-28T11:46:04.965056Z"
    }
   },
   "outputs": [],
   "source": [
    "# import inspect\n",
    "# f=(nn.BatchNorm2d)\n",
    "# inspect.signature(f).parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-28T11:46:05.059383Z",
     "start_time": "2020-05-28T11:46:05.011747Z"
    }
   },
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Classifier, self).__init__()\n",
    "        # torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        # torch.nn.MaxPool2d(kernel_size, stride, padding)\n",
    "        # input 維度 [3, 128, 128]\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 3, 1, 1),  # [64, 128, 128]\n",
    "            nn.BatchNorm2d(num_features=64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0),      # [64, 64, 64]\n",
    "\n",
    "            nn.Conv2d(64, 128, 3, 1, 1), # [128, 64, 64]\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0),      # [128, 32, 32]\n",
    "\n",
    "            nn.Conv2d(128, 256, 3, 1, 1), # [256, 32, 32]\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0),      # [256, 16, 16]\n",
    "\n",
    "            nn.Conv2d(256, 512, 3, 1, 1), # [512, 16, 16]\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0),       # [512, 8, 8]\n",
    "            \n",
    "            nn.Conv2d(512, 512, 3, 1, 1), # [512, 8, 8]\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0),       # [512, 4, 4]\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(512*4*4, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 11)\n",
    "        )\n",
    "\n",
    "    def forward(self, in_):\n",
    "        x = self.cnn(in_)\n",
    "        print(x.shape)\n",
    "        x = x.reshape(x.shape[0], -1) # x.shape = x.sise()\n",
    "        out_ = self.fc(x)\n",
    "        return out_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-28T11:46:09.178296Z",
     "start_time": "2020-05-28T11:46:05.063198Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Classifier().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-07T14:38:17.925646Z",
     "start_time": "2020-06-07T14:38:17.921078Z"
    }
   },
   "outputs": [],
   "source": [
    "# y = model(x) \n",
    "# y = model.__call__(x)\n",
    "# y = model.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-28T11:46:09.185856Z",
     "start_time": "2020-05-28T11:46:09.180719Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier(\n",
      "  (cnn): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU()\n",
      "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (8): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): ReLU()\n",
      "    (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (12): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (14): ReLU()\n",
      "    (15): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (16): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (17): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (18): ReLU()\n",
      "    (19): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=8192, out_features=1024, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=11, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-28T11:46:09.285228Z",
     "start_time": "2020-05-28T11:46:09.187671Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 128, 128]           1,792\n",
      "       BatchNorm2d-2         [-1, 64, 128, 128]             128\n",
      "              ReLU-3         [-1, 64, 128, 128]               0\n",
      "         MaxPool2d-4           [-1, 64, 64, 64]               0\n",
      "            Conv2d-5          [-1, 128, 64, 64]          73,856\n",
      "       BatchNorm2d-6          [-1, 128, 64, 64]             256\n",
      "              ReLU-7          [-1, 128, 64, 64]               0\n",
      "         MaxPool2d-8          [-1, 128, 32, 32]               0\n",
      "            Conv2d-9          [-1, 256, 32, 32]         295,168\n",
      "      BatchNorm2d-10          [-1, 256, 32, 32]             512\n",
      "             ReLU-11          [-1, 256, 32, 32]               0\n",
      "        MaxPool2d-12          [-1, 256, 16, 16]               0\n",
      "           Conv2d-13          [-1, 512, 16, 16]       1,180,160\n",
      "      BatchNorm2d-14          [-1, 512, 16, 16]           1,024\n",
      "             ReLU-15          [-1, 512, 16, 16]               0\n",
      "        MaxPool2d-16            [-1, 512, 8, 8]               0\n",
      "           Conv2d-17            [-1, 512, 8, 8]       2,359,808\n",
      "      BatchNorm2d-18            [-1, 512, 8, 8]           1,024\n",
      "             ReLU-19            [-1, 512, 8, 8]               0\n",
      "        MaxPool2d-20            [-1, 512, 4, 4]               0\n",
      "           Linear-21                 [-1, 1024]       8,389,632\n",
      "             ReLU-22                 [-1, 1024]               0\n",
      "           Linear-23                  [-1, 512]         524,800\n",
      "             ReLU-24                  [-1, 512]               0\n",
      "           Linear-25                   [-1, 11]           5,643\n",
      "================================================================\n",
      "Total params: 12,833,803\n",
      "Trainable params: 12,833,803\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.19\n",
      "Forward/backward pass size (MB): 49.59\n",
      "Params size (MB): 48.96\n",
      "Estimated Total Size (MB): 98.73\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "summary(model, input_size=(3, 128, 128))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-28T11:46:09.303486Z",
     "start_time": "2020-05-28T11:46:09.288790Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mappingproxy({'params': <Parameter \"params\">,\n",
       "              'lr': <Parameter \"lr=0.001\">,\n",
       "              'betas': <Parameter \"betas=(0.9, 0.999)\">,\n",
       "              'eps': <Parameter \"eps=1e-08\">,\n",
       "              'weight_decay': <Parameter \"weight_decay=0\">,\n",
       "              'amsgrad': <Parameter \"amsgrad=False\">})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import inspect\n",
    "inspect.signature(torch.optim.Adam).parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-28T13:09:29.538123Z",
     "start_time": "2020-05-28T13:02:17.574751Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "epoch = 1, time = 88, train_loss = 0.043, train_acc = 0.26, valid_loss = 0.040, valid_acc = 0.30\n",
      "epoch = 2, time = 87, train_loss = 0.036, train_acc = 0.37, valid_loss = 0.040, valid_acc = 0.30\n",
      "epoch = 3, time = 85, train_loss = 0.033, train_acc = 0.43, valid_loss = 0.038, valid_acc = 0.36\n",
      "epoch = 4, time = 86, train_loss = 0.030, train_acc = 0.48, valid_loss = 0.031, valid_acc = 0.47\n",
      "epoch = 5, time = 84, train_loss = 0.028, train_acc = 0.52, valid_loss = 0.030, valid_acc = 0.49\n"
     ]
    }
   ],
   "source": [
    "print('train')\n",
    "\n",
    "model = Classifier().cuda()\n",
    "epochs = 5\n",
    "\n",
    "# initialize loss\n",
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "# initialize update_weight\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # initialize time\n",
    "    epoch_start_time = time.time()\n",
    "    # initialize metric\n",
    "    train_acc = 0.\n",
    "    valid_acc = 0.\n",
    "    train_loss = 0.\n",
    "    valid_loss = 0.\n",
    "    \n",
    "    # -----------------------\n",
    "    #  train \n",
    "    # -----------------------\n",
    "    model.train() ###\n",
    "    for (i, data) in enumerate(train_loader):\n",
    "        x, y = data[0].cuda(), data[1].cuda()\n",
    "        \n",
    "        # reset gradient !!!\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # y_hat, (-1,11)\n",
    "        y_hat = model(x)\n",
    "        \n",
    "        # loss\n",
    "        batch_loss = loss(y_hat, y)\n",
    "        \n",
    "        # gradient\n",
    "        batch_loss.backward()\n",
    "        \n",
    "        # update_weight\n",
    "        optimizer.step()\n",
    "        \n",
    "        # metric, acc, loss\n",
    "        label_hat = np.argmax(y_hat.cpu().data.numpy(), axis=1)\n",
    "        label = y.cpu().data.numpy()\n",
    "        match = (label_hat==label)\n",
    "        train_acc += sum(match)\n",
    "        train_loss += batch_loss.item()\n",
    "        \n",
    "\n",
    "    \n",
    "    # -----------------------\n",
    "    #  valid \n",
    "    # -----------------------\n",
    "    model.eval() ###\n",
    "    with torch.no_grad(): ###\n",
    "        for (i, data) in enumerate(valid_loader):\n",
    "            x, y = data[0].cuda(), data[1].cuda()\n",
    "\n",
    "            # y_hat\n",
    "            y_hat = model(x)\n",
    "\n",
    "            # loss\n",
    "            batch_loss = loss(y_hat, y)\n",
    "\n",
    "            # metric\n",
    "            label_hat = np.argmax(y_hat.cpu().data.numpy(), axis=1)\n",
    "            label = y.cpu().data.numpy()\n",
    "            match = (label_hat==label)\n",
    "            valid_acc += sum(match)\n",
    "\n",
    "            valid_loss += batch_loss.item()\n",
    "    \n",
    "    \n",
    "    # -----------------------\n",
    "    #  progress \n",
    "    # -----------------------\n",
    "    epoch += 1\n",
    "    t = time.time() - epoch_start_time\n",
    "    train_loss /= len(train_set)\n",
    "    valid_loss /= len(valid_set)\n",
    "    train_acc /= len(train_set)\n",
    "    valid_acc /= len(valid_set)\n",
    "\n",
    "    print('epoch = %d, time = %d, train_loss = %.3f, train_acc = %.2f, valid_loss = %.3f, valid_acc = %.2f'%(\n",
    "        epoch, t, train_loss, train_acc, valid_loss, valid_acc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-28T12:25:46.193639Z",
     "start_time": "2020-05-28T12:25:46.188619Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model_dict = model.state_dict()\n",
    "# for key in model_dict:\n",
    "#     val = model_dict[key]\n",
    "#     print('%-30s %-30s'%(key, val.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-28T13:09:29.851266Z",
     "start_time": "2020-05-28T13:09:29.540868Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), './best_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-28T13:09:29.869497Z",
     "start_time": "2020-05-28T13:09:29.857035Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.1079, -0.2099, -0.0213], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(model.state_dict()['cnn.0.weight'][0,0,0,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-28T13:12:54.443909Z",
     "start_time": "2020-05-28T13:12:54.295523Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.1091, -0.0359, -0.1072], device='cuda:0')\n",
      "tensor([-0.1079, -0.2099, -0.0213], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "best_model = Classifier().cuda()\n",
    "print(best_model.state_dict()['cnn.0.weight'][0,0,0,:])\n",
    "\n",
    "best_model.load_state_dict(torch.load('./best_model.pt'))\n",
    "print(best_model.state_dict()['cnn.0.weight'][0,0,0,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# infer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-28T13:14:27.280925Z",
     "start_time": "2020-05-28T13:14:27.272898Z"
    }
   },
   "outputs": [],
   "source": [
    "test_set = ImgDataset(x_test_paths, y_train, test_transform)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-28T13:18:33.463125Z",
     "start_time": "2020-05-28T13:18:10.576708Z"
    }
   },
   "outputs": [],
   "source": [
    "predict = []\n",
    "\n",
    "best_model.eval()\n",
    "with torch.no_grad():\n",
    "    for (i, data) in enumerate(valid_loader):\n",
    "        x = data[0].cuda()\n",
    "        y_hat = model(x)\n",
    "        y_hat = y_hat.cpu().data.numpy()\n",
    "        predict.append(y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-28T13:19:25.641170Z",
     "start_time": "2020-05-28T13:19:25.627225Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3430, 11), (3430,))"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = np.concatenate(predict, axis=0)\n",
    "result_label = np.argmax(result, axis=1)\n",
    "\n",
    "result.shape, result_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
